{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saitejasri1/Shared-ML-project/blob/final-from-prit/merged1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13tX-6f5azRm",
        "outputId": "5db89d60-9aab-4246-edd8-6a18c08a6da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCZ6vyL6aw5V"
      },
      "outputs": [],
      "source": [
        "!pip install paddlepaddle-gpu==2.3.0 -i https://mirror.baidu.com/pypi/simple\n",
        "!pip install paddleocr\n",
        "# git clone https://github.com/PaddlePaddle/PaddleOCR\n",
        "!pip install PaddleOCR\n",
        "!pip install langdetect\n",
        "!pip install paddlepaddle\n",
        "\n",
        "#German to English\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "from paddleocr import PaddleOCR\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "import logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozBkMOyHa3gG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlr4FXmQa6e_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from sklearn.metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "hunwZY24hK6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix27X74mXmET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce48735d-c172-450a-ee0e-a614f6d165a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7806fd768430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7806fd769bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 536ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 942ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
            "output_Ensemble-> cucumber,eggplant,mango,bell pepper,capsicum,garlic,cabbage,soy beans\n",
            "detected_ingredients_flat-> ['beans,sugar']\n",
            "Recommended Recipes Based on Your Ingredients:\n",
            "                                                     title  \\\n",
            "1271502                                  Bibimbap At Home    \n",
            "1278565    Homemade French Fries With Five Dipping Sauces    \n",
            "1478977   Basic Chili Paste To Replace Chili Powder Recipe   \n",
            "1480622    Roosevelt Avenue-Style Cemita Sandwiches Recipe   \n",
            "1334794                               Assorted Vegetables    \n",
            "1290280  Fruit Salad With Herb, Citrus, Mint-Maple, Or ...   \n",
            "1478657                             Grilled Oysters Recipe   \n",
            "1362065                    Slow-Cooked Texas Beer Brisket    \n",
            "1360411  Strip Steak With Roasted Acorn Squash And Spro...   \n",
            "1486292  Choucroute Garnie À L'Alsacienne (Alsatian Bra...   \n",
            "1486246       Falafel With Black Olives And Harissa Recipe   \n",
            "1320799                               Shrimp & Cumin Soup    \n",
            "1271311                         Dulce De Leche Half-Moons    \n",
            "1308313                             Farmers Market Greens    \n",
            "1349148                         Grilled Brined Vegetables    \n",
            "1285456                             Shellfish Mixed Grill    \n",
            "1362303      Pan-Grilled Black Bass With Flavored Butters    \n",
            "1359739                             Brandied Giblet Gravy    \n",
            "1314732                                 Grilled Flatbread    \n",
            "1362485  Quinoa Salad With Toasted Nuts And Seeds, Salt...   \n",
            "\n",
            "                                               ingredients  \\\n",
            "1271502  [1/2 cup reduced-sodium soy sauce, 1/3 cup fin...   \n",
            "1278565  [3 pounds russet (baking) potatoes (5 or 6), 4...   \n",
            "1478977  [3 whole sweet fresh dried chilies, such as, ,...   \n",
            "1480622  [4, or store-bought cemita buns (see note), se...   \n",
            "1334794  [1 lb small red potatoes, 1 lb asparagus, trim...   \n",
            "1290280  [1 pint strawberries, hulled and halved, 1 hal...   \n",
            "1478657  [1 dozen oysters, 4 to 5 ounces compound butte...   \n",
            "1362065  [1 (7 to 9 pound) beef brisket, untrimmed, 1/2...   \n",
            "1360411  [1 (12-ounce) boneless strip steak, Kosher sal...   \n",
            "1486292  [1 pound (450g) boneless pork loin, 1 pound (4...   \n",
            "1486246  [1 recipe, 3 tablespoons (45ml), or, 3 ounces ...   \n",
            "1320799  [1 tablespoon vegetable oil, 1 medium onion, f...   \n",
            "1271311  [1/2 cup raw green (hulled) pumpkin seeds, Pow...   \n",
            "1308313  [1 tablespoon Champagne vinegar, 1/2 tablespoo...   \n",
            "1349148  [2 bay leaves, 10 black peppercorns, 1 cup uns...   \n",
            "1285456  [Two 1 1/2-pound lobsters, bodies and tails sp...   \n",
            "1362303  [1 pound broccolini or broccoli rabe, trimmed,...   \n",
            "1359739  [Giblets from one (18- to 20-pound) turkey, 1 ...   \n",
            "1314732  [2 1/2 teaspoons active dry yeast, 4 3/4 cups ...   \n",
            "1362485  [a\\tA1/2 cup quinoa., a\\t50g natural almond., ...   \n",
            "\n",
            "                                                directions  \\\n",
            "1271502  [\"Whisk first 6 ingredients in a medium bowl. ...   \n",
            "1278565  [\"Peel potatoes, then cut lengthwise into 1/3-...   \n",
            "1478977  [\"Place chilies on a microwave-safe plate and ...   \n",
            "1480622  [\"Preheat oven to 350\\u00b0F. Heat 2 cast-iron...   \n",
            "1334794  [\"Cover potatoes with\", \"by 1 inch in a 4-quar...   \n",
            "1290280  [\"Combine all the fruit in a large bowl and st...   \n",
            "1478657  [\"Line a rimmed baking sheet with heavy-duty a...   \n",
            "1362065  [\"Grilling Method: Indirect/Medium-Low Heat\", ...   \n",
            "1360411  [\"Heat a dry medium skillet, preferably cast i...   \n",
            "1486292  [\"Generously season pork loin and shoulder all...   \n",
            "1486246  [\"Complete\", \"recipe through the end of step 2...   \n",
            "1320799  [\"Heat oil in a large skillet over medium heat...   \n",
            "1271311  [\"Preheat oven to 350\\u00b0F with rack in midd...   \n",
            "1308313  [\"Whisk together vinegar, shallot, salt, and p...   \n",
            "1349148  [\"Bring bay leaves, peppercorns, vinegar, suga...   \n",
            "1285456  [\"Light a grill or preheat a grill pan. Arrang...   \n",
            "1362303  [\"Cook broccolini in a pot of boiling salted w...   \n",
            "1359739  [\"Trim and discard all tough cartilage from gi...   \n",
            "1314732  [\"Dissolve yeast in 3 cups warm water in a lar...   \n",
            "1362485  [\"Method\", \"Place the quinoa in pot with 1 cup...   \n",
            "\n",
            "                                                      link  \n",
            "1271502  www.epicurious.com/recipes/food/views/bibimbap...  \n",
            "1278565  www.epicurious.com/recipes/food/views/homemade...  \n",
            "1478977  www.seriouseats.com/recipes/2015/01/chili-pure...  \n",
            "1480622  www.seriouseats.com/recipes/2015/04/roosevelt-...  \n",
            "1334794  www.epicurious.com/recipes/food/views/assorted...  \n",
            "1290280  www.epicurious.com/recipes/food/views/fruit-sa...  \n",
            "1478657  www.seriouseats.com/recipes/2016/07/grilled-oy...  \n",
            "1362065  www.epicurious.com/recipes/food/views/slow-coo...  \n",
            "1360411  www.epicurious.com/recipes/food/views/strip-st...  \n",
            "1486292  www.seriouseats.com/recipes/2017/01/choucroute...  \n",
            "1486246  www.seriouseats.com/recipes/2016/03/falafel-bl...  \n",
            "1320799  www.epicurious.com/recipes/member/views/shrimp...  \n",
            "1271311  www.epicurious.com/recipes/food/views/dulce-de...  \n",
            "1308313  www.epicurious.com/recipes/food/views/farmers-...  \n",
            "1349148  www.epicurious.com/recipes/food/views/grilled-...  \n",
            "1285456  www.epicurious.com/recipes/food/views/shellfis...  \n",
            "1362303  www.epicurious.com/recipes/food/views/pan-gril...  \n",
            "1359739  www.epicurious.com/recipes/food/views/brandied...  \n",
            "1314732  www.epicurious.com/recipes/food/views/grilled-...  \n",
            "1362485  www.epicurious.com/recipes/member/views/quinoa...  \n"
          ]
        }
      ],
      "source": [
        "# Suppress debug messages from ppocr\n",
        "logging.getLogger(\"ppocr\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
        "\n",
        "def process_images_and_detect_ingredients(N_ensemble, N_OCR, ensemble_image_paths, ocr_image_paths):\n",
        "    \"\"\"\n",
        "    Process images using Ensemble model and detect ingredients using PaddleOCR.\n",
        "\n",
        "    Args:\n",
        "    N_ensemble (int): Number of times to run the Ensemble process.\n",
        "    N_OCR (int): Number of times to run the OCR process.\n",
        "    ensemble_image_paths (list): List of paths to the images for Ensemble model.\n",
        "    ocr_image_paths (list): List of paths to the images for PaddleOCR.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - str: Comma-separated output keywords for Ensemble model.\n",
        "        - list: List of strings containing comma-separated detected ingredients for each OCR image.\n",
        "    \"\"\"\n",
        "    # Ensemble model\n",
        "    def process_images(N, image_paths):\n",
        "        results = []\n",
        "\n",
        "        # Load pre-trained models\n",
        "        VGG16 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/VGG16.keras')\n",
        "        DenseNet121 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/InceptionV3.keras')\n",
        "        InceptionV3 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/InceptionV3.keras')\n",
        "\n",
        "        for path in image_paths:\n",
        "            input_image = tf.io.read_file(path)\n",
        "            image = tf.image.decode_image(input_image, channels=3)\n",
        "\n",
        "            image = tf.image.resize(image, size=(224, 224))\n",
        "            image = image / 255.0\n",
        "            image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "            pred_probs_VGG16 = VGG16.predict(image)\n",
        "            pred_probs_DenseNet121 = DenseNet121.predict(image)\n",
        "            pred_probs_InceptionV3 = InceptionV3.predict(image)\n",
        "\n",
        "            ensemble_pred = np.maximum.reduce([pred_probs_VGG16, pred_probs_DenseNet121, pred_probs_InceptionV3])\n",
        "\n",
        "            predicted_label = np.argmax(ensemble_pred)\n",
        "            pred_prob = ensemble_pred[0][predicted_label]\n",
        "\n",
        "            # Output label handling\n",
        "            class_names = ['apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum', 'carrot', 'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant',\n",
        "                           'garlic', 'ginger', 'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce', 'mango', 'onion', 'orange', 'paprika', 'pear', 'peas',\n",
        "                           'pineapple', 'pomegranate', 'potato', 'raddish', 'soy beans',\n",
        "                           'spinach', 'sweetcorn', 'sweetpotato', 'tomato', 'turnip', 'watermelon']  # Define class names here\n",
        "            if predicted_label < len(class_names):\n",
        "              output_label = class_names[predicted_label]\n",
        "            else:\n",
        "              output_label = \"Unknown\"\n",
        "            results.append(output_label)  # Append the output label to results list\n",
        "\n",
        "        return ','.join(results)  # Join all output labels with commas and return\n",
        "\n",
        "    # PaddleOCR model\n",
        "    def detect_ingredients_from_images(N, image_paths):\n",
        "        # Setup OCR model with English language\n",
        "        ocr_model = PaddleOCR(lang='en')\n",
        "\n",
        "        # Define the list of ingredients\n",
        "        ingredients = {'beans', 'salt', 'butter', 'sugar', 'onion', 'water', 'eggs', 'oliveoil', 'flour', 'milk',\n",
        "                       'garliccloves', 'pepper', 'brownsugar', 'garlic', 'all-purposeflour', 'bakingpowder', 'egg',\n",
        "                       'saltandpepper', 'parmesancheese', 'lemonjuice', 'bakingsoda', 'vegetableoil', 'vanilla',\n",
        "                       'blackpepper', 'cinnamon', 'tomatoes', 'sourcream', 'garlicpowder', 'vanillaextract', 'oil',\n",
        "                       'honey', 'onions', 'creamcheese', 'garlicclove', 'celery', 'cheddarcheese', 'unsaltedbutter',\n",
        "                       'soysauce'}\n",
        "\n",
        "        # Initialize an empty list to store results for each image\n",
        "        all_detected_ingredients = []\n",
        "\n",
        "        for _ in range(N):\n",
        "            # Initialize an empty list to store detected ingredients for this iteration\n",
        "            detected_ingredients_iteration = []\n",
        "\n",
        "            for image_path in image_paths:\n",
        "                # Check if the image is in HEIC format\n",
        "                if image_path.lower().endswith('.heic'):\n",
        "                    # Convert HEIC image to PNG format\n",
        "                    heic_img = Image.open(image_path)\n",
        "                    image_np = np.array(heic_img.convert('RGB'))\n",
        "                else:\n",
        "                    # Load the image\n",
        "                    image = Image.open(image_path)\n",
        "                    # Convert image to numpy array\n",
        "                    image_np = np.array(image)\n",
        "\n",
        "                # Perform OCR on the image\n",
        "                result = ocr_model.ocr(image_np)\n",
        "\n",
        "                # Extract text from OCR result\n",
        "                text = \"\"\n",
        "                if result is not None:\n",
        "                    for line in result:\n",
        "                        for word in line:\n",
        "                            text += word[1][0] + ' '\n",
        "                else:\n",
        "                    print(\"No text detected in image:\", image_path)\n",
        "\n",
        "                # Check if the detected text is in English\n",
        "                if detect(text) != 'en':\n",
        "                    # Translate text to English\n",
        "                    translator = Translator()\n",
        "                    translated = translator.translate(text, src='auto', dest='en')\n",
        "                    text = translated.text\n",
        "\n",
        "                # Convert text to lowercase and split into words\n",
        "                text = text.lower().split()\n",
        "\n",
        "                # Find intersection of detected ingredients and predefined ingredients list\n",
        "                detected_ingredients = ingredients.intersection(text)\n",
        "\n",
        "                # Join detected ingredients into a comma-separated string\n",
        "                detected_ingredients_str = ','.join(detected_ingredients)\n",
        "\n",
        "                # Add detected ingredients string to the list for this iteration\n",
        "                detected_ingredients_iteration.append(detected_ingredients_str)\n",
        "\n",
        "            # Add the list of detected ingredients for this iteration to the main list\n",
        "            all_detected_ingredients.append(detected_ingredients_iteration)\n",
        "\n",
        "        return all_detected_ingredients\n",
        "\n",
        "    # Execute Ensemble model\n",
        "    output_Ensemble = process_images(N_ensemble, ensemble_image_paths)\n",
        "\n",
        "    # Execute PaddleOCR model\n",
        "    detected_ingredients = detect_ingredients_from_images(N_OCR, ocr_image_paths)\n",
        "\n",
        "    return output_Ensemble, detected_ingredients\n",
        "\n",
        "# RecipeNLG model\n",
        "def load_data(filepath):\n",
        "    \"\"\" Load the dataset and preprocess the NER column \"\"\"\n",
        "    data = pd.read_csv(filepath)\n",
        "    data['ingredients'] = data['ingredients'].apply(eval)  # Convert ingredients from string to list\n",
        "    return data\n",
        "\n",
        "def build_tfidf_model(data):\n",
        "    \"\"\" Build and return a TF-IDF model and matrix \"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['ingredients'])\n",
        "    return tfidf_vectorizer, tfidf_matrix\n",
        "\n",
        "def find_similar_recipes(user_input, tfidf_vectorizer, tfidf_matrix, data):\n",
        "    \"\"\" Find and return similar recipes based on user input \"\"\"\n",
        "    # Transform user input using the same tfidf vectorizer\n",
        "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
        "    # Calculate cosine similarities between user input and all recipes\n",
        "    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "    # Get the indices of recipes sorted by the number of ingredient matches\n",
        "    sorted_indices = cosine_similarities.argsort()[0][::-1]\n",
        "    # Return top 20 similar recipes with max n matches\n",
        "    similar_recipes = data.iloc[sorted_indices[:20]][['title', 'ingredients', 'directions', 'link']]\n",
        "    return similar_recipes\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths to images for Ensemble model\n",
        "    ensemble_image_paths = [\"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/0.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/1.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/2.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/3.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/4.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/5.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/6.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/7.jpg\"]\n",
        "\n",
        "    # Paths to images for PaddleOCR model\n",
        "    ocr_image_paths = [\"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/old/1.jpg\",\n",
        "                       \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/old/2.jpg\"]\n",
        "    # Execute Ensemble and PaddleOCR models\n",
        "    output_Ensemble, detected_ingredients = process_images_and_detect_ingredients(1, 1, ensemble_image_paths, ocr_image_paths)\n",
        "\n",
        "    # Load the dataset for RecipeNLG\n",
        "    filepath = '/content/drive/MyDrive/do-not-delete/ML-Project/RecipeNLG/RecipeNLG-all.csv'\n",
        "    data = load_data(filepath)\n",
        "\n",
        "    # Build the TF-IDF model and matrix\n",
        "    tfidf_vectorizer, tfidf_matrix = build_tfidf_model(data)\n",
        "\n",
        "    # # Flatten the list of lists into a single list of strings\n",
        "    # detected_ingredients_flat = [','.join(ingredients) for ingredients in detected_ingredients]\n",
        "\n",
        "    # Merge output keywords from Ensemble and detected ingredients from PaddleOCR\n",
        "    # print(\"output_Ensemble->\",output_Ensemble)\n",
        "    # print(\"detected_ingredients_flat->\",detected_ingredients_flat)\n",
        "    # user_input = output_Ensemble + ',' + ','.join(detected_ingredients_flat)\n",
        "\n",
        "    # Flatten the list of lists into a single list of strings\n",
        "    detected_ingredients_flat = [','.join(ingredients) for ingredients in detected_ingredients]\n",
        "\n",
        "    # Merge output keywords from Ensemble and detected ingredients from PaddleOCR\n",
        "    output_Ensemble_str = ','.join(output_Ensemble)\n",
        "    print(\"output_Ensemble_str->\",output_Ensemble_str)\n",
        "    detected_ingredients_flat_str = ','.join(detected_ingredients_flat)\n",
        "    print(\"detected_ingredients_flat_str->\",detected_ingredients_flat_str)\n",
        "    user_input = output_Ensemble_str + ',' + detected_ingredients_flat_str\n",
        "    print(\"user_input->\",user_input)\n",
        "\n",
        "    # Find similar recipes\n",
        "    similar_recipes = find_similar_recipes(user_input, tfidf_vectorizer, tfidf_matrix, data)\n",
        "    print(\"Recommended Recipes Based on Your Ingredients:\")\n",
        "    print(similar_recipes)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5Ha4wu0vPf4sQUAqde6nB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}