{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQKPfZUvSYNUr/gXqw5OLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saitejasri1/Shared-ML-project/blob/final-from-prit/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "13tX-6f5azRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle-gpu==2.3.0 -i https://mirror.baidu.com/pypi/simple\n",
        "!pip install paddleocr\n",
        "# git clone https://github.com/PaddlePaddle/PaddleOCR\n",
        "!pip install PaddleOCR\n",
        "!pip install langdetect\n",
        "!pip install paddlepaddle\n",
        "\n",
        "#German to English\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "YCZ6vyL6aw5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "ozBkMOyHa3gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlr4FXmQa6e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix27X74mXmET"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import models\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import *\n",
        "from sklearn.metrics import *\n",
        "from paddleocr import PaddleOCR\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "import logging\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Suppress debug messages from ppocr\n",
        "logging.getLogger(\"ppocr\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
        "\n",
        "def process_images_and_detect_ingredients(N_ensemble, N_OCR, ensemble_image_paths, ocr_image_paths):\n",
        "    \"\"\"\n",
        "    Process images using Ensemble model and detect ingredients using PaddleOCR.\n",
        "\n",
        "    Args:\n",
        "    N_ensemble (int): Number of times to run the Ensemble process.\n",
        "    N_OCR (int): Number of times to run the OCR process.\n",
        "    ensemble_image_paths (list): List of paths to the images for Ensemble model.\n",
        "    ocr_image_paths (list): List of paths to the images for PaddleOCR.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - str: Comma-separated output keywords for Ensemble model.\n",
        "        - list: List of strings containing comma-separated detected ingredients for each OCR image.\n",
        "    \"\"\"\n",
        "    # Ensemble model\n",
        "    def process_images(N, image_paths):\n",
        "        results = []\n",
        "\n",
        "        # Load pre-trained models\n",
        "        VGG16 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/VGG16.keras')\n",
        "        DenseNet121 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/InceptionV3.keras')\n",
        "        InceptionV3 = load_model('/content/drive/MyDrive/do-not-delete/ML-Project/InceptionV3.keras')\n",
        "\n",
        "        for path in image_paths:\n",
        "            input_image = tf.io.read_file(path)\n",
        "            image = tf.image.decode_image(input_image, channels=3)\n",
        "\n",
        "            image = tf.image.resize(image, size=(224, 224))\n",
        "            image = image / 255.0\n",
        "            image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "            pred_probs_VGG16 = VGG16.predict(image)\n",
        "            pred_probs_DenseNet121 = DenseNet121.predict(image)\n",
        "            pred_probs_InceptionV3 = InceptionV3.predict(image)\n",
        "\n",
        "            ensemble_pred = np.maximum.reduce([pred_probs_VGG16, pred_probs_DenseNet121, pred_probs_InceptionV3])\n",
        "\n",
        "            predicted_label = np.argmax(ensemble_pred)\n",
        "            pred_prob = ensemble_pred[0][predicted_label]\n",
        "\n",
        "            class_names = test.class_names\n",
        "            output_label = class_names[predicted_label]\n",
        "            results.append(output_label)  # Append the output label to results list\n",
        "\n",
        "        return ','.join(results)  # Join all output labels with commas and return\n",
        "\n",
        "    # PaddleOCR model\n",
        "    def detect_ingredients_from_images(N, image_paths):\n",
        "        # Setup OCR model with English language\n",
        "        ocr_model = PaddleOCR(lang='en')\n",
        "\n",
        "        # Define the list of ingredients\n",
        "        ingredients = {'beans', 'salt', 'butter', 'sugar', 'onion', 'water', 'eggs', 'oliveoil', 'flour', 'milk',\n",
        "                       'garliccloves', 'pepper', 'brownsugar', 'garlic', 'all-purposeflour', 'bakingpowder', 'egg',\n",
        "                       'saltandpepper', 'parmesancheese', 'lemonjuice', 'bakingsoda', 'vegetableoil', 'vanilla',\n",
        "                       'blackpepper', 'cinnamon', 'tomatoes', 'sourcream', 'garlicpowder', 'vanillaextract', 'oil',\n",
        "                       'honey', 'onions', 'creamcheese', 'garlicclove', 'celery', 'cheddarcheese', 'unsaltedbutter',\n",
        "                       'soysauce'}\n",
        "\n",
        "        # Initialize an empty list to store results for each image\n",
        "        all_detected_ingredients = []\n",
        "\n",
        "        for _ in range(N):\n",
        "            # Initialize an empty list to store detected ingredients for this iteration\n",
        "            detected_ingredients_iteration = []\n",
        "\n",
        "            for image_path in image_paths:\n",
        "                # Check if the image is in HEIC format\n",
        "                if image_path.lower().endswith('.heic'):\n",
        "                    # Convert HEIC image to PNG format\n",
        "                    heic_img = Image.open(image_path)\n",
        "                    image_np = np.array(heic_img.convert('RGB'))\n",
        "                else:\n",
        "                    # Load the image\n",
        "                    image = Image.open(image_path)\n",
        "                    # Convert image to numpy array\n",
        "                    image_np = np.array(image)\n",
        "\n",
        "                # Perform OCR on the image\n",
        "                result = ocr_model.ocr(image_np)\n",
        "\n",
        "                # Extract text from OCR result\n",
        "                text = \"\"\n",
        "                if result is not None:\n",
        "                    for line in result:\n",
        "                        for word in line:\n",
        "                            text += word[1][0] + ' '\n",
        "                else:\n",
        "                    print(\"No text detected in image:\", image_path)\n",
        "\n",
        "                # Check if the detected text is in English\n",
        "                if detect(text) != 'en':\n",
        "                    # Translate text to English\n",
        "                    translator = Translator()\n",
        "                    translated = translator.translate(text, src='auto', dest='en')\n",
        "                    text = translated.text\n",
        "\n",
        "                # Convert text to lowercase and split into words\n",
        "                text = text.lower().split()\n",
        "\n",
        "                # Find intersection of detected ingredients and predefined ingredients list\n",
        "                detected_ingredients = ingredients.intersection(text)\n",
        "\n",
        "                # Join detected ingredients into a comma-separated string\n",
        "                detected_ingredients_str = ','.join(detected_ingredients)\n",
        "\n",
        "                # Add detected ingredients string to the list for this iteration\n",
        "                detected_ingredients_iteration.append(detected_ingredients_str)\n",
        "\n",
        "            # Add the list of detected ingredients for this iteration to the main list\n",
        "            all_detected_ingredients.append(detected_ingredients_iteration)\n",
        "\n",
        "        return all_detected_ingredients\n",
        "\n",
        "    # Execute Ensemble model\n",
        "    output_Ensemble = process_images(N_ensemble, ensemble_image_paths)\n",
        "\n",
        "    # Execute PaddleOCR model\n",
        "    detected_ingredients = detect_ingredients_from_images(N_OCR, ocr_image_paths)\n",
        "\n",
        "    return output_Ensemble, detected_ingredients\n",
        "\n",
        "# RecipeNLG model\n",
        "def load_data(filepath):\n",
        "    \"\"\" Load the dataset and preprocess the NER column \"\"\"\n",
        "    data = pd.read_csv(filepath)\n",
        "    data['ingredients'] = data['ingredients'].apply(eval)  # Convert ingredients from string to list\n",
        "    return data\n",
        "\n",
        "def build_tfidf_model(data):\n",
        "    \"\"\" Build and return a TF-IDF model and matrix \"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(data['ingredients'])\n",
        "    return tfidf_vectorizer, tfidf_matrix\n",
        "\n",
        "def find_similar_recipes(user_input, tfidf_vectorizer, tfidf_matrix, data):\n",
        "    \"\"\" Find and return similar recipes based on user input \"\"\"\n",
        "    # Transform user input using the same tfidf vectorizer\n",
        "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
        "    # Calculate cosine similarities between user input and all recipes\n",
        "    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "    # Get the indices of recipes sorted by the number of ingredient matches\n",
        "    sorted_indices = cosine_similarities.argsort()[0][::-1]\n",
        "    # Return top 20 similar recipes with max n matches\n",
        "    similar_recipes = data.iloc[sorted_indices[:20]][['title', 'ingredients', 'directions', 'link']]\n",
        "    return similar_recipes\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths to images for Ensemble model\n",
        "    ensemble_image_paths = [\"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/1.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/2.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/3.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/4.jpg\",\n",
        "                            \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/Ensemble/5.jpg\"]\n",
        "\n",
        "    # Paths to images for PaddleOCR model\n",
        "    ocr_image_paths = [\"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/1.jpg\",\n",
        "                       \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/2.jpg\",\n",
        "                       \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/2.jpg\",\n",
        "                       \"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/2.jpg\",\n",
        "                       ,\"/content/drive/MyDrive/do-not-delete/ML-Project/Images/for_OCR/2.jpg\"]\n",
        "\n",
        "    # Execute Ensemble and PaddleOCR models\n",
        "    output_Ensemble, detected_ingredients = process_images_and_detect_ingredients(1, 1, ensemble_image_paths, ocr_image_paths)\n",
        "\n",
        "    # Load the dataset for RecipeNLG\n",
        "    filepath = '/content/drive/MyDrive/do-not-delete/ML-Project/RecipeNLG/RecipeNLG-all.csv'\n",
        "    data = load_data(filepath)\n",
        "\n",
        "    # Build the TF-IDF model and matrix\n",
        "    tfidf_vectorizer, tfidf_matrix = build_tfidf_model(data)\n",
        "\n",
        "    # Merge output keywords from Ensemble and detected ingredients from PaddleOCR\n",
        "    user_input = output_Ensemble + ',' + ','.join(detected_ingredients)\n",
        "\n",
        "    # Find similar recipes\n",
        "    similar_recipes = find_similar_recipes(user_input, tfidf_vectorizer, tfidf_matrix, data)\n",
        "    print(\"Recommended Recipes Based on Your Ingredients:\")\n",
        "    print(similar_recipes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5F_WaFuas6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GC7HQ7nTar-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}